---
title: "Red-list African elephant dynamics (Savannah)"
author: "Charles T T Edwards (Wellington, New Zealand)"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: no
vignette: >
  %\VignetteIndexEntry{savannah}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path = 'fig/savannah-', fig.width = 6, tidy = TRUE, tidy.opts = list(blank = TRUE, width.cutoff = 95), message = FALSE, warning = FALSE, collapse = TRUE, comment = "#>")

options(rmarkdown.html_vignette.check_title = FALSE)

suppressPackageStartupMessages(library(rstan))
suppressPackageStartupMessages(library(rstudioapi))
options(mc.cores = parallel::detectCores())
suppressPackageStartupMessages(library(pander))
suppressPackageStartupMessages(library(plyr))
```

Load the package and extract stan model code
```{r}
library(red)

reg_ll <- stan_model(model_code = model_code("slope"), model_name = "log-linear")
reg_lp <- stan_model(model_code = model_code("poly"),  model_name = "log-polynomial")
```

Data are stored in the \code{redData} auxilliary package.

```{r}
data(savannah, package = "redData")
```

## Log-linear model fit

Get initial values, run the model and examine posterior predictive fits to the data.
```{r}
# get initial values
ini <- with(savannah, list(x0_log = rep(-1, sum(S)), 
            alpha0 = 0,
            alphaC = rep(0, C),
            alphaS = rep(0, sum(S)),
            sigma = 1, tau = c(0.1,0.1)))

reg_map <- optimizing(reg_ll, savannah, init = function() ini, as_vector = FALSE)

reg_ini         <- list()
reg_ini$x0_log  <- reg_map$pars$x0_log
reg_ini$alpha0  <- reg_map$pars$alpha0
reg_ini$alphaC  <- reg_map$pars$alphaC
reg_ini$alphaS  <- reg_map$pars$alphaS
reg_ini$sigma   <- 1
reg_ini$tau     <- c(0.1,0.1)
```

```{r, results = 'hide'}
# MCMC sampling
reg_ll_out <- sampling(reg_ll, data = savannah, init = function() reg_ini, chains = 2) 
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
gg <- traceplot(reg_ll_out, pars = c("parameter_summary", "error_summary", "output_summary"))
gg <- gg + facet_wrap(~parameter, scales = "free_y") + theme_bw(base_size = 12) + guides(col = FALSE)
gg <- gg + ggtitle("Trace summary statistics")
print(gg)
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
dat$density_sim <- posterior(reg_ll_out, "y_sim", fun = "median")[[1]]

gg <- ggplot(dat) + 
    geom_point(aes(x = density, y = density_sim)) + 
    facet_wrap(~country) +
    labs(x = "Observed", y = "Predicted") + 
    geom_abline(intercept = 0, slope = 1) +
    theme_bw(base_size = 14) +
    scale_y_log10() + scale_x_log10() +
    ggtitle("Posterior prediction of survey density data")
print(gg)
```

## Log-polynomial model fit
Get initial values, run the model and examine posterior predictive fits to the data.
```{r}
# get initial values
ini <- with(savannah, list(x0_log = rep(-1, sum(S)), 
            alpha0 = 0,
            alphaC = rep(0, C),
            alphaS = rep(0, sum(S)),
            beta0 = 0,
            betaC = rep(0, C),
            betaS = rep(0, sum(S)),
            sigma = 1, tau = c(0.1,0.1,0.1,0.1)))

reg_map <- optimizing(reg_lp, savannah, init = function() ini, as_vector = FALSE)

reg_ini         <- list()
reg_ini$x0_log  <- reg_map$pars$x0_log
reg_ini$alpha0  <- reg_map$pars$alpha0
reg_ini$alphaC  <- reg_map$pars$alphaC
reg_ini$alphaS  <- reg_map$pars$alphaS
reg_ini$beta0   <- reg_map$pars$beta0
reg_ini$betaC   <- reg_map$pars$betaC
reg_ini$betaS   <- reg_map$pars$betaS
reg_ini$sigma   <- 1
reg_ini$tau     <- c(0.1,0.1,0.1,0.1)
```

```{r, results = 'hide'}
# MCMC sampling
reg_lp_out <- sampling(reg_lp, data = savannah, init = function() reg_ini, chains = 2) 
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
gg <- traceplot(reg_lp_out, pars = c("parameter_summary", "error_summary", "output_summary"))
gg <- gg + facet_wrap(~parameter, scales = "free_y") + theme_bw(base_size = 12) + guides(col = FALSE)
gg <- gg + ggtitle("Trace summary statistics")
print(gg)
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
dat$density_sim <- posterior(reg_lp_out, "y_sim", fun = "median")[[1]]

gg <- ggplot(dat) + 
    geom_point(aes(x = density, y = density_sim)) + 
    facet_wrap(~country) +
    labs(x = "Observed", y = "Predicted") + 
    geom_abline(intercept = 0, slope = 1) +
    theme_bw(base_size = 14) +
    scale_y_log10() + scale_x_log10() +
    ggtitle("Posterior prediction of survey density data")
print(gg)
```

## Compare predictions

Predictive abilities of the models are compared using the mean absolute prediction error:
$$
MPE = \frac{1}{n}\sum{\frac{|\hat{y}_i - y_i|}{y_i}}
$$
where $y_i$ is the observation and $\hat{y}_i$ is the posterior predicted value. Because the MPE distributions are similar between the two model fits we conclude that the polynomial model does not provide any improvement in terms of predictive ability, and select the log-linear model as the most parsimonious representation of the data.

```{r}
dfr_ll <- posterior(reg_ll_out, pars = "MPE", melt = TRUE)[[1]]
dfr_lp <- posterior(reg_lp_out, pars = "MPE", melt = TRUE)[[1]]

dfr <- merge(dfr_ll, dfr_lp, by = "iterations", suffixes = c(".ll", ".lp"))
dfr <- reshape2::melt(dfr, id.vars = "iterations")

gg <- ggplot(dfr) +
    geom_density(aes(value, fill = variable), alpha = 0.3) + labs(x = "Mean absolute prediction error", y = "") +
    theme_bw(base_size = 16)
print(gg)
```

# Trend estimates

## Expected posterior decline
The model is constructed under an assumption that each site represents a random sample of the decline from a panmitic population with correlated dynamics between sites. This allows information to be shared across sites in a statistically coherent manner (and is necessary given the low number of surveys per site). The decline is extracted from the model by integrating across sites to obtain an expectation across the population. The result is ecologically relevant, since each site across the whole population contributes to the estimated decline. The contribution of each site to the final estimate is in proportion to the information content of the data at that site.

The decline is esimated as:
$$
\mathrm{decline} = 1 - \exp(\alpha_0 \cdot T + \tau^2 / 2)
$$
where $\alpha_0$ is the intercept of the growth rates per site, $T$ is the number of years over which the decline is being estimated and $\tau^2$ is the variance of the declines per site. 

The posterior distribution of the expected decline is:

```{r}
dfr <- posterior(reg_ll_out, pars = "expected_decline", melt = TRUE)[[1]]

gg <- ggplot(dfr) + 
  geom_histogram(aes(value, ..density..), binwidth = 0.1) +
  theme_bw(base_size = 20) %+replace% theme(axis.text.y = element_blank()) +
  scale_x_continuous(limits = c(-0.5, 1.2)) +
  labs(x = "Posterior Decline", y = "") +
  geom_vline(xintercept = c(0.2, 0.5, 0.8, 1.0), col = "red")
print(gg)
```

We can plot the distribution of growth rates per site ($\alpha_0 + \varepsilon_{i}$, where $\varepsilon_i$ is a random effect per site):
```{r, echo = FALSE, fig.width=10, fig.height=10}
gg <- plot(reg_ll_out, pars = "alpha_re") + theme(axis.text.y = element_blank()) + ggtitle("Growth rate per site")
gg <- gg + geom_vline(xintercept = posterior(reg_ll_out, "alpha0", fun = "median")[[1]], col = "blue", size = 2, alpha = 0.6)
print(gg)

```

and tabulate the results:
```{r, echo = FALSE, results = "asis"}
dfr <- posterior(reg_ll_out, pars = "expected_decline", melt = TRUE)[[1]]
  
ff <- function(x) paste0(round(median(x, na.rm = TRUE), 2), " (", round(quantile(x, 0.025, na.rm = TRUE), 2), " - ", round(quantile(x, 0.975, na.rm = TRUE), 2), ")")

out <-  with(dfr, data.frame(
             decline = ff(value), 
             critical   = round(mean(value > 0.8),2), 
             endangered = round(mean(value <= 0.8 & value > 0.5),2), 
             vulnerable = round(mean(value <= 0.5 & value > 0.2),2), 
             not_threatened = round(mean(value <= 0.2),2)))
pandoc.table(out, style = "rmarkdown")
```

## Numbers weighted average decline
Estimation of the decline as an arithmetic average is presented as an alternative approach. Creating an arithmetic average of the decline across sites is equivalent to assuming that each site represents an independent population i.e. a fixed effect. This is inconsistent with how the model is constructed and is unlikely to be true biologically because of shared biological characteristics of each population, movement between sites and common anthropogenic threats. The method stipulates that a numbers weighted average of the decline per site is constructed. This is despite an absence of data concerning the numbers at each site at a common reference point in time. Instead, the density is converted into a numbers estimate using an assumed range area. The range area is obtained from the modal survey area at that site. The survey area, as recorded in the data, is determined by a variety of factors, including financial constraints and the expected number of elephants at that particular site, and therefore has only a very weak relationship to the range size. These factors have also likely changed over time in unknown ways. Converting density into numbers is therefore difficult. In summary, creating a numbers weighted average is statistically dubious, relies on data that doesn't exist, and furthermore creates a result that will be heavily biased towards the populations assumed to be largest. Ecological extinction could occur across much of the populations range and yet this method could still return an optimistic result if a single large and increasing population dominates the result.

The numbers per site is obtained from model estimates of the log-density $x$ at site $i$ and time $t$, the observation error variance per site $\sigma^2$, the assumed range area size $a_i$ and the maximum likelihood estimate of the probability of a positive survey $\theta_i$:
$$
n_{i,t} = \theta_i \cdot \exp{(x_{i,t} + \sigma^2 / 2)}\cdot a_i
$$

The decline per site is then:
$$
\mathrm{decline\,per\,site} = 1 - \frac{n_{i,t}}{n_{i,t=0}}
$$

and the numbers weighted average of the decline is:
$$
\mathrm{decline} = \frac{\sum{(n_{i, t=0} \cdot \mathrm{decline\,per\,site}})}{\sum{n_{i, t=0}}}
$$

The posterior distribution of the average decline is:
```{r}
dfr <- posterior(reg_ll_out, pars = "decline", melt = TRUE)[[1]]

gg <- ggplot(dfr) + 
  geom_histogram(aes(value, ..density..), binwidth = 0.1) +
  theme_bw(base_size = 20) %+replace% theme(axis.text.y = element_blank()) +
  scale_x_continuous(limits = c(-0.5, 1.2)) +
  labs(x = "Average Decline", y = "") +
  geom_vline(xintercept = c(0.2, 0.5, 0.8, 1.0), col = "red")
print(gg)
```

We can also plot the decline per site ($1 - \frac{n_{i,t}}{n_{i,t=0}}$):
```{r, echo = FALSE, fig.width=10, fig.height=10}
gg <- plot(reg_ll_out, pars = "decline_site") + theme(axis.text.y = element_blank()) + ggtitle("Numbers decline per site")
gg <- gg + geom_vline(xintercept = posterior(reg_ll_out, "decline", fun = "median")[[1]], col = "blue", size = 2, alpha = 0.6)
print(gg)
```

Assuming a numbers weighted average of the decline per site it is relevant to generate a table of the declines per site, in this case taking median values per site:
```{r, results = "asis"}
year_N0 <- 1965

nit <- 2000

zones <- row.names(savannah$survey_area)
  
lookup <- dlply(dat, .(zone), summarize, as.character(unique(country)))
lookup <- lapply(lookup, function(x) x[,1])

country_lookup <- function(zone) {
    
    lookup[[as.character(zone)]]
}


N0 <- posterior(reg_ll_out, pars = "N0", melt = TRUE, fun = "median", dim.names = list(list(iter = 1:nit, zone = zones)))[[1]]
NT <- posterior(reg_ll_out, pars = "NT", melt = TRUE, fun = "median", dim.names = list(list(iter = 1:nit, zone = zones)))[[1]]
ds <- posterior(reg_ll_out, pars = "decline_site", melt = TRUE, fun = "median", dim.names = list(list(iter = 1:nit, zone = zones)))[[1]]

dfr <- cbind(N0, NT, ds)
colnames(dfr) <- c(paste0("N_", year_N0), "N_2015", "decline")
dfr$zone <- rownames(dfr)
dfr$country <- vapply(dfr$zone, country_lookup, character(1))

dfr2 <- data.frame(range_area = savannah$range_area, zone = zones)

dfr <- merge(dfr, dfr2, by = "zone")

dfr <- dfr[order(dfr$country),]
row.names(dfr) <- NULL
pandoc.table(dfr[, c(5, 1, 2, 3, 4, 6)], style = "rmarkdown", split.tables = Inf)
```

```{r, echo = FALSE}
plot(1 - N_2015 / N_1965 ~ decline, dfr)
abline(0,1)
```

From the table we can extract an estimate of the decline:
```{r}
with(dfr, sum(N_1984 * decline) / sum(N_1984))
```

and tabulate the results:
```{r, results = "asis"}
dfr <- posterior(reg_ll_out, pars = "decline", melt = TRUE)[[1]]
  
ff <- function(x) paste0(round(median(x, na.rm = TRUE), 2), " (", round(quantile(x, 0.025, na.rm = TRUE), 2), " - ", round(quantile(x, 0.975, na.rm = TRUE), 2), ")")

out <-  with(dfr, data.frame(
             decline = ff(value), 
             critical   = round(mean(value > 0.8),2), 
             endangered = round(mean(value <= 0.8 & value > 0.5),2), 
             vulnerable = round(mean(value <= 0.5 & value > 0.2),2), 
             not_threatened = round(mean(value <= 0.2),2)))
pandoc.table(out, style = "rmarkdown")
```

